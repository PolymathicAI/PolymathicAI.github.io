---
layout: post
title: "Contextual Counting: A Mechanistic Study of Transformers on a Quantitative Task"
authors: Siavash Golkar, Alberto Bietti, Mariel Pettee, Michael Eickenberg, Miles Cranmer, Keiya Hirashima, Geraud Krawezik, Nicholas Lourie, Michael McCabe, Rudy Morel, Ruben Ohana, Liam Holden Parker, Bruno RÃ©galdo-Saint Blancard, Kyunghyun Cho, Shirley&nbsp;Ho
shorttitle: "Contextual Counting: A Mechanistic Study"
date: 2024-05-30 09:23
image: counting-splash.jpg
smallimage: counting-s.jpg
blurb: We introduce the Contextual Counting task, a new toy problem aimed at exploring interpretability of Transformer models in quantitative domains. We compare the performance of causal and non-causal models with different position codes and find causal models with RoPE and NoPE significantly outperform other configurations. We provide detailed explanation of how the circuits function and what makes them succeed or fail in generalization to out-of-distribution samples.
shortblurb: We introduce the Contextual Counting task, a new toy problem aimed at exploring interpretability of Transformer models in quantitative domains.
splashimage: /images/blog/counting-splash.jpg
link: 
github_link: 
permalink: /blog/contextual_counting/
---

One of our goals at Polymathic-AI is to utilize the recent advances in machine learning to implement and deploy state of the art models that can aid in scientific exploration and discovery. As such, we believe that it is very important for us to be able to understand how these models behave and interpret the algorithms that are learned in these networks. The benefits of this direction of research are two-fold. First, by understanding the inner workings of the model, we get better insight into the scientific domain of interest. Second, by understanding the strengths and weaknesses of these architectures, we can design networks that are better suited for their task.

In this blog post, we summarize a recent paper which is part of an ongoing effort in our team in this direction. In this work, we introduced a new toy problem specifically designed to advance the interpretability of Transformer models in quantitative and scientific contexts. This task, called **contextual counting**, requires the model to identify a specific region of interest within a dataset and perform accurate counting. As such, it simulates scenarios where precise localization and subsequent computation are critical, such as in object detection or region-based analysis in scientific data. 


## Introducing the Contextual Counting Task
<br>
In this task, the input is a sequence composed of zeros, ones, and square bracket delimiters: `{0, 1, [, ]}`. Each sample sequence contains ones and zeros with several regions marked by the delimiters. The task is to count the number of ones within each delimited region. For example, given the sequence:

```
input = [ 0 ] [ 1 0 1 ] 0 [ 1 ] 1 [ ] 0
```

The target output would be:

```
target = [0, 2, 1, 0]
```

For simplicity, the regions are non-overlapping.

To tackle this task using a Transformer architecture, we use an encoder-decoder setup and provide the decoder with a fixed prompt that labels the regions. For the example above, the prompt would be:

```
prompt = [0, 1, 2, 3]
```

For our experiments, we fix the number of regions to 4 and the sequence length to 512. This allows us to explore how solutions generalize to different numbers of regions and sequence lengths.

#### Relevance

The contextual counting task is not just an instructive example for understanding Transformers; it emulates real-world quantitative problems requiring precise sensitivity to regional boundaries. Examples include counting specific neuro-receptors within a neuron in biological research.

Current state-of-the-art LLMs struggle with this task, indicating the need for specialized models and interpretability techniques tailored to quantitative and scientific applications.

#### Why Study Toy Problems?

Toy problems serve as simplified models that help us understand complex systems. By stripping down the intricacies of real-world scenarios, toy problems allow researchers to isolate and examine specific mechanisms within machine learning models. This focused approach is particularly valuable in the study of interpretability, where the goal is to unravel how models make decisions. Simplified tasks such as contextual counting provide a clear, controlled environment where researchers can systematically manipulate variables and observe the effects. This clarity is often lost in more complex, real-world problems, where numerous interacting factors can obscure the underlying processes. By starting with toy problems, we gain foundational insights that can later be applied to more complicated tasks.

Moreover, toy problems are instrumental in benchmarking and testing new theories and methods. They act as proving grounds for hypotheses about model behavior and performance. For instance, by using toy problems, researchers can quickly iterate on models and interpretability techniques, refining their approaches before deploying them on more sophisticated and critical tasks. This iterative process accelerates the development of robust methods that can be confidently applied in high-stakes domains like healthcare, finance, and scientific research. In the context of Transformers, toy problems help uncover how different architectures and encoding methods influence model performance and interpretability, providing essential knowledge for advancing machine learning technologies.


## Theoretical Insights
<br>
We provide some theoretical insights into the problem, showing that a Transformer with one causal encoding layer and one decoding layer can solve the contextual counting task for arbitrary sequence lengths and numbers of regions.

#### Contextual Position (CP)

Contextual position refers to positional information in a sequence that is meaningful only within the context of the problem. For the contextual counting task, this means knowing the region number for each token. For example, with three regions, the input and contextual positions might look like:

```
input = 0 1 1 [ 1 0 1 ] 0 [ 1 ] 1 [ ] 0
CP =    - - - 1 1 1 1 1 - 2 2 2 - 3 3 -
```

This information helps disambiguate the different regions based on context.

#### Key Propositions

1. **Proposition 1:** If the regional contextual position information is available in the latent representation of the tokens at some layer of a Transformer, the contextual counting task can be solved with a single additional layer.
2. **Proposition 2:** A causal Transformer with a single layer and no position encoding (NoPE) can infer the regional contextual position.

These propositions imply that a two-layer causal Transformer with NoPE can solve the contextual counting task.

#### Challenges for Non-Causal Transformers

For non-causal (bidirectional) Transformers, the task is more complicated:

- **Proposition 3:** A non-causal Transformer with no position code and a permutation-invariant output head cannot solve the contextual counting task.
- **Proposition 4:** To emulate a causal attention profile, a non-causal attention layer with Absolute Position code would need an embedding space at least as large as the sequence length.

These propositions highlight the difficulties non-causal Transformers face in solving this task.



## Experimental Results
<br>
The theoretical results above imply that exact solutions exist but do not clarify whether or not such solutions can indeed be found when the model is trained via SGD. We therefore trained various Transformer architectures on this task. Inspired by the theoretical arguments, we use an encoder-decoder architecture, with one layer and one head for each. A typical output of the network is shown in the following image where the model outputs the probability distribution over the number of ones in each region.

<p align="center">
  <img src="/images/blog/counting/output.png" alt="Typical output of the model" width="55%" style="mix-blend-mode: darken;">
</p>


We summarize the results of this empirical exploration below.

#### 1. Causal transformers significantly outperform non-causal ones.

<p align="center">
  <img src="/images/blog/counting/accuracy.png" alt="Performance of the different configuraiton" width="55%" style="mix-blend-mode: darken;">
</p>

The above figure shows the performance of different Transformer configurations. The most prominant feature of this figure is that non-causal transformers with any positional encoding fail to get good performance. In contrast, causal Transformers can achieve close to 100\% accuracy.

#### 2. NoPE is best but harder to train than RoPE.


We also see that the very best model is trained with NoPE but RoPE is much more consistent in training.

#### 3. In the best performing models, the encoder captures the regional contextual position information.

As described above, the regional contextual position is an important piece of information for this task. Looking at the projection of the 1-token embeddings in the different regions, we see that this information is accurately captured. 

<p align="center">
  <img src="/images/blog/counting/pca_proj.png" alt="PCA projection of the 1-tokens after the encoder layer." width="45%" style="mix-blend-mode: darken;">
</p>

By looking at the details of the attention module of the encoder, we see that in causal models, this information is inferred by attending to all the previous delimiter tokens equally. Each token can tell which region it is in by looking at how many delimiter tokens of each kind preceded it.


#### 4. In the best performing models, the decoder attends only to the 1-tokens in the relevant region.

We can verify explicitly that the inferred regional contextual position in the encoder is used in the decoder cross-attention module such that the attention profile is focused on the 1-tokens of the relevant region (in the below figure, the third region).

<p align="center">
  <img src="/images/blog/counting/decoder.png" alt="The attention profile of the decoder." width="45%" style="mix-blend-mode: darken;">
</p>

We see that in this example, the decoder also attends to the beginning-of-sequence token. The reason for this is that, if the model *only* attends to the 1-tokens, then the number of the 1-tokens - the quantity of interest - is going to cancel in the calculation of the softmax. However, if there is another token, then the number of 1-tokens will be preserved. In this way, this other token acts as a bias term when computing the output of the attention module. 

#### 6. Out-of-distribution generalization is directly linked to which tokens are used as bias terms.

The figure below shows the behavior of three different type of solutions when generalizing to sequences of different lengths and inputs with different number of regions. Even though all three attain the same performance on the in-distribution data, their out-of-distribution performance is very different. Why is this the case?

<p align="center">
  <img src="/images/blog/counting/var_sols.png" alt="Different types of solutions." width="95%" style="mix-blend-mode: darken;">
</p>

We can get a hint at what might be the culprit by looking at the attention pattern of the decoder. The attention pattern given in the previous point pertains to the blue dots on this figure, i.e. the model that generalizes best. 

The figure below, shows the attention pattern of the orange dots, i.e. the model that generalizes do different seuqence lengths but not to different region numbers. We see that as before, the decoder pays attention to the 1-tokens of the relevant region (in this case the first region), however this time the role of the bias term is played by the ]-tokens. During training, the number of regions is fixed at 4, and therefore the number of ]-tokens can be used as a constant bias. However, this is not the case when the number of regions changes. This explains why this model does not generalize to other number of regions.

<p align="center">
  <img src="/images/blog/counting/decoder_nongen.png" alt="The attention profile of the decoder of a non-generalizing model." width="45%" style="mix-blend-mode: darken;">
</p>

In our exploration, we found that the model can use any combination of quantities that are constant during training as biases. 

#### 7. (Technical) The network generates its output by balancing two learned shapes.

This point is a little technical and it pertains to the detail of how the network explicitly generates its output. I think it is cute enough to be worth mentioning. 

In some of our experiments, we chose to remove the MLP and self-attention layers from the decoder block. That is, the decoder is just a cross-attention layer. This configuration is less expressive but has the advantage that the output of the model is a linear combination of the value vectors derived from the embeddings of the encoder.

In a preious case we saw that the decoder only attended to the 1-tokens of the relevant region and the beginning-of-sequence token. The figure below shows the value vectors of these two tokens.

<p align="center">
  <img src="/images/blog/counting/values.png" alt="The value vectors." width="55%" style="mix-blend-mode: darken;">
</p>

We can verify that by adding n-times the value vector of the 1-token to the value vector of the BoS-token, we arrive at a distribution that (after a softmax) is peaked at n. Comparing this with the output of the model, we see that this is indeed what the network is implementing.

<p align="center">
  <img src="/images/blog/counting/formula.png" alt="The value vectors." width="55%" style="mix-blend-mode: darken;">
</p>

Therefore, in these models we fully understand what attention patterns the model is using, how these attention patterns are implemented and explicitly how the output of the network is constructed.

If you made it this far, here is an interesting bonus point:

* Even though the model has access to the number n through its attention profile, it still does not construct a probability distribution that is sharply peaked at n. As we see in the above figure, as n gets large, this probability distribution gets wider. This, we believe is partly the side-effect of this specific solution where two curves are being balanced against each other. But it is partly a general problem that as the number of tokens that are attended to gets large, we need higher accuracy to be able to infer n exactly. This is because the information about n is coded non-linearly after the attention layer. In this case, if we assume that the model attends to BoS and 1-tokens equally the output becomes:

<p align="center">
  <img src="/images/blog/counting/n_dependence.png" alt="The n-dependence of the model output." width="55%" style="mix-blend-mode: darken;">
</p>

We see that as n becomes large, the difference between n and n+1 becomes smaller.


## Conclusion
<br>
The contextual counting task provides a valuable framework for exploring the interpretability of Transformers in scientific and quantitative contexts. Our experiments show that causal Transformers with NoPE can effectively solve this task, while non-causal models struggle. These findings highlight the importance of task-specific interpretability challenges and the potential for developing more robust and generalizable models for scientific applications.

For more details, check out our preprint on the [arXiv](link).

*-- Siavash Golkar*


Image by [Tim Mossholder](https://unsplash.com/photos/blue-and-black-electric-wires-FwzhysPCQZc) via Unsplash.